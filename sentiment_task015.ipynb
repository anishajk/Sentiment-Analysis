{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpbLHvauHNV2"
      },
      "source": [
        "Homework 4: Sentiment Analysis - Task 0, Task 1, Task 5 (all primarily written tasks)\n",
        "----\n",
        "\n",
        "The following instructions are only written in this notebook but apply to all notebooks and `.py` files you submit for this homework.\n",
        "\n",
        "Due date: October 25th, 2023\n",
        "\n",
        "Points:\n",
        "- Task 0: 5 points\n",
        "- Task 1: 10 points\n",
        "- Task 2: 30 points\n",
        "- Task 3: 20 points\n",
        "- Task 4: 20 points\n",
        "- Task 5: 15 points\n",
        "\n",
        "Goals:\n",
        "- understand the difficulties of counting and probablities in NLP applications\n",
        "- work with real world data to build a functioning language model\n",
        "- stress test your model (to some extent)\n",
        "\n",
        "Complete in groups of: __two (pairs)__. If you prefer to work on your own, you may, but be aware that this homework has been designed as a partner project.\n",
        "\n",
        "Allowed python modules:\n",
        "- `numpy`, `matplotlib`, `keras`, `pytorch`, `nltk`, `pandas`, `sci-kit learn` (`sklearn`), `seaborn`, and all built-in python libraries (e.g. `math` and `string`)\n",
        "- if you would like to use a library not on this list, post on piazza to request permission\n",
        "- all *necessary* imports have been included for you (all imports that we used in our solution)\n",
        "\n",
        "Instructions:\n",
        "- Complete outlined problems in this notebook.\n",
        "- When you have finished, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__.\n",
        "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
        "- Double check that you have completed Task 0.\n",
        "- Submit your work on Gradescope.\n",
        "- Double check that your submission on Gradescope looks like you believe it should __and__ that all partners are included (for partner work).\n",
        "\n",
        "6120 students: complete __all__ problems.\n",
        "\n",
        "4120 students: you are not required to complete problems marked \"CS 6120 REQUIRED\". If you complete these you will not get extra credit. We will not take points off if you attempt these problems and do not succeed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m86eezdHHNV5"
      },
      "source": [
        "Names & Sections\n",
        "----\n",
        "Names:\n",
        "Anisha Kushwaha 6120\n",
        "Sarthak Kagliwal 6120 (Write these in every notebook you submit. For each partner, write down whether you are a 4120 or a 6120 student.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6uuvTa1HNV5"
      },
      "source": [
        "Task 0: Name, References, Reflection (5 points)\n",
        "---\n",
        "\n",
        "References\n",
        "---\n",
        "List the resources you consulted to complete this homework here. Write one sentence per resource about what it provided to you. If you consulted no references to complete your assignment, write a brief sentence stating that this is the case and why it was the case for you.\n",
        "\n",
        "https://www.nltk.org/_modules/nltk/metrics/scores.html\n",
        "  - To check how nltk metrics work\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "  - Parameters of LR model\n",
        "https://www.geeksforgeeks.org/naive-bayes-classifiers/#\n",
        "  - Naive bayes works\n",
        "AI Collaboration\n",
        "---\n",
        "Following the *AI Collaboration Policy* in the syllabus, please cite any LLMs that you used here and briefly describe what you used them for. Additionally, provide comments in-line identifying the specific sections that you used LLMs on, if you used them towards the generation of any of your answers.\n",
        "\n",
        "__NEW__: Do not include nested list comprehensions supplied by AI collaborators â€” all nested lists comprehensions __must__ be re-written.\n",
        "\n",
        "Reflection\n",
        "----\n",
        "Answer the following questions __after__ you complete this assignment (no more than 1 sentence per question required, this section is graded on completion):\n",
        "\n",
        "1. Does this work reflect your best effort?\\\n",
        "Answer: Yes, it reflects the best effort\n",
        "2. What was/were the most challenging part(s) of the assignment?\\\n",
        "Answer: Plotting the graph with different percentage of training data.\n",
        "3. If you want feedback, what function(s) or problem(s) would you like feedback on and why?\\\n",
        "Answer: All the graphs plotted\n",
        "4. Briefly reflect on how your partnership functioned--who did which tasks, how was the workload on each of you individually as compared to the previous homeworks, etc.\\\n",
        "Answer: Sarthak: NN and LR models training and\n",
        " Anisha: NB and the utils file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6__Wzl8HNV5"
      },
      "source": [
        "Task 1: Provided Data Write-Up (10 points)\n",
        "---\n",
        "\n",
        "Every time you use a data set in an NLP application (or in any software application), you should be able to answer a set of questions about that data. Answer these now. Default to no more than 1 sentence per question needed. If more explanation is necessary, do give it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKgZcHhzHNV6"
      },
      "source": [
        "This is about the __provided__ movie review data set.\n",
        "\n",
        "1. Where did you get the data from? The provided dataset(s) were sub-sampled from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "2. (1 pt) How was the data collected (where did the people acquiring the data get it from and how)?\\\n",
        "Answer: The data was collected from the Kaggle dataset, which was originally sourced from the IMDB website and contains movie reviews.\n",
        "\n",
        "3. (2 pts) How large is the dataset (answer for both the train and the dev set, separately)? (# reviews, # tokens in both the train and dev sets)\\\n",
        "Answer:The dataset consists of 50,000 movie reviews, for train size is 1600 reviews (425421 tokens) and dev is 200 reviews (54603 tokens).\n",
        "4. (1 pt) What is your data? (i.e. newswire, tweets, books, blogs, etc)\\\n",
        "Answer: The data in the IMDB-50K dataset consists of movie reviews.\n",
        "\n",
        "5. (1 pt) Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people)\\\n",
        "Answer: Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher\n",
        "6. (2 pts) What is the distribution of labels in the data (answer for both the train and the dev set, separately)?\\\n",
        "Answer: Train Data -- 0: 796, 1: 804,  Dev data -- 0:95, 1: 105\n",
        "\n",
        "7. (2 pts) How large is the vocabulary (answer for both the train and the dev set, separately)? train vocab 30705, Dev vocab 8953\n",
        "8. (1 pt) How big is the overlap between the vocabulary for the train and dev set? 6574"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VEyLOJFnHNV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14bf208-bf86-4d5c-d866-32c60e3acc76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Token:  425421\n",
            "Dev Token:  54603\n"
          ]
        }
      ],
      "source": [
        "# our utility functions\n",
        "# RESTART your jupyter notebook kernel if you make changes to this file\n",
        "import sentiment_utils as sutils\n",
        "import numpy as np\n",
        "\n",
        "# define constants for the files we are using\n",
        "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
        "DEV_FILE = \"movie_reviews_dev.txt\"\n",
        "\n",
        "# load in your data and make sure you understand the format\n",
        "# Do not print out too much so as to impede readability of your notebook\n",
        "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
        "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n",
        "\n",
        "train_tokens = 0\n",
        "dev_token = 0\n",
        "for review in train_tups[0]:\n",
        "  train_tokens += len(review)\n",
        "\n",
        "for review in dev_tups[0]:\n",
        "  dev_token += len(review)\n",
        "\n",
        "print(\"Train Token: \", train_tokens)\n",
        "print(\"Dev Token: \", dev_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uo7nK_QvHNV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f27d9a6-aa8d-4fef-cb4c-aa23197ac1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data:  [796 804]\n",
            "Dev data:  [ 95 105]\n"
          ]
        }
      ],
      "source": [
        "# Feel free to write code to help answer the above questions\n",
        "print(\"train data: \", np.bincount(np.array(train_tups[1])))\n",
        "print(\"Dev data: \", np.bincount(np.array(dev_tups[1])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_vocab = sutils.create_index(train_tups[0])\n",
        "dev_vocab = sutils.create_index(dev_tups[0])\n",
        "print(\"Train vocab:\", len(train_vocab))\n",
        "print(\"Dev vocab:\", len(dev_vocab))\n",
        "\n",
        "overlap = list(set(train_vocab) & set(dev_vocab))\n",
        "print(\"Overlap len: \",len(overlap))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4kFUh8lE9SU",
        "outputId": "91a7ccd9-2bfd-40dc-fcc2-4ef884702da7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train vocab: 30705\n",
            "Dev vocab: 8953\n",
            "Overlap len:  6574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8iuyRn7HNV7"
      },
      "source": [
        "Task 5: Model Evaluation (15 points)\n",
        "---\n",
        "Save your three graph files for the __best__ configurations that you found with your models using the `plt.savefig(filename)` command. The `bbox_inches` optional parameter will help you control how much whitespace outside of the graph is in your resulting image.\n",
        "Run your each notebook containing a classifier 3 times, resulting in __NINE__ saved graphed (don't just overwrite your previous ones).\n",
        "\n",
        "You will turn in all of these files.\n",
        "\n",
        "10 points in this section are allocated for having all nine graphs legible, properly labeled, and present.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlnxukjuHNV7"
      },
      "source": [
        "\n",
        "\n",
        "1. (1 pt) When using __10%__ of your data, which model had the highest f1 score?\n",
        "  Neural Network\n",
        "2. (1 pt) Which classifier had the most __consistent__ performance (that is, which classifier had the least variation across all three graphs you have for it -- no need to mathematically calculate this, you can just look at the graphs)?  Naive Bayes\n",
        "3. (1 pt) For each model, what percentage of training data resulted in the highest f1 score?\n",
        "    1. Naive Bayes: 80%\n",
        "    2. Logistic Regression: 60%\n",
        "    3. Neural Net: 70%\n",
        "4. (2 pts) Which model, if any, appeared to overfit the training data the most? Why?\n",
        "\n",
        "  Answer: Neural Network, as it's metrics(graph) are very low, which indicates poor performance on dev data compared to training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F8ZiL-QHNV7"
      },
      "source": [
        "6120 REQUIRED\n",
        "----\n",
        "\n",
        "Find a second data set that is labeled for sentiment from a different domain (not movie reivews). Rerun your notebook with this data (you should set up your notebook so that you only need to change the paths and possibly run a different pre-processing function on the data). Note that you will want binary labels.\n",
        "\n",
        "Answer the regular data questions for your new data set\n",
        "----\n",
        "1. Where did you get the data from?\n",
        "  Kaggle\n",
        "2. How was the data collected (where did the people acquiring the data get it from and how)?\n",
        "  The collected it from twitter\n",
        "3. How large is the dataset (answer for both the train and the dev set, separately)? (# reviews, # tokens in both the train and dev sets)\n",
        "  Train data: 1200 reviews, 18792 tokens,  Dev data: 300 review, 4740 tokens\n",
        "4. What is your data? (i.e. newswire, tweets, books, blogs, etc)\n",
        "  tweets\n",
        "5. Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people)\n",
        "  It was from Kaggle\n",
        "6. What is the distribution of labels in the data (answer for both the train and the dev set, separately)? Train data- 0:810, 1:390,  Dev data- 0: 211 1: 89\n",
        "7. How large is the vocabulary (answer for both the train and the dev set, separately)? Train: 4205, Dev: 1552\n",
        "8. How big is the overlap between the vocabulary for the train and dev set?\n",
        "848\n",
        "Answer the model evaluation questions for your new data set\n",
        "----\n",
        "1. When using __10%__ of your data, which model had the highest f1 score?\n",
        "  Neural Network\n",
        "2. Which classifier had the most __consistent__ performance (that is, which classifier had the least variation across all three graphs you have for it -- no need to mathematically calculate this, you can just look at the graphs)?\n",
        "  Naive bayes\n",
        "3. For each model, what percentage of training data resulted in the highest f1 score?\n",
        "    1. Naive Bayes: 90%\n",
        "    2. Logistic Regression: 100%\n",
        "    3. Neural Net: 60%\n",
        "4. Which model, if any, appeared to overfit the training data the most? Why?\n",
        "   \n",
        "    Neural Network, as it's metrics(graph) are very low, which indicates poor performance on dev data compared to training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KpojMCfuHNV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a039f8-7fd5-40c2-e33a-f96c879f2222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Token:  18792\n",
            "Dev Token:  4740\n"
          ]
        }
      ],
      "source": [
        "# any code you need to write here\n",
        "OW_TRAIN_FILE = \"own_train.txt\"\n",
        "OW_DEV_FILE = \"own_dev.txt\"\n",
        "\n",
        "# load in your data and make sure you understand the format\n",
        "# Do not print out too much so as to impede readability of your notebook\n",
        "ow_train_tups = sutils.generate_tuples_from_file(OW_TRAIN_FILE)\n",
        "ow_dev_tups = sutils.generate_tuples_from_file(OW_DEV_FILE)\n",
        "\n",
        "ow_train_tokens = 0\n",
        "ow_dev_token = 0\n",
        "for review in ow_train_tups[0]:\n",
        "  ow_train_tokens += len(review)\n",
        "\n",
        "for review in ow_dev_tups[0]:\n",
        "  ow_dev_token += len(review)\n",
        "\n",
        "print(\"Train Token: \", ow_train_tokens)\n",
        "print(\"Dev Token: \", ow_dev_token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train len own data:\", len(ow_train_tups[0]))\n",
        "print(\"Dev len own data:\", len(ow_dev_tups[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rHByj6prctG",
        "outputId": "b38b6980-8d5e-40f8-b267-60e3b69de87c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train len own data: 1200\n",
            "Dev len own data: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train data: \", np.bincount(np.array(ow_train_tups[1])))\n",
        "print(\"Dev data: \", np.bincount(np.array(ow_dev_tups[1])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYvhRuVDqX8E",
        "outputId": "f90e3d5e-6471-49f4-e621-d23f8ac57e05"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data:  [810 390]\n",
            "Dev data:  [211  89]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ow_train_vocab = sutils.create_index(ow_train_tups[0])\n",
        "ow_dev_vocab = sutils.create_index(ow_dev_tups[0])\n",
        "print(\"Train vocab:\", len(ow_train_vocab))\n",
        "print(\"Dev vocab:\", len(ow_dev_vocab))\n",
        "\n",
        "ow_overlap = list(set(ow_train_vocab) & set(ow_dev_vocab))\n",
        "print(\"Overlap len: \",len(ow_overlap))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSzPkwwyqY2Q",
        "outputId": "c0b5ab47-81bb-4b84-c59f-c8f941327c33"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train vocab: 4205\n",
            "Dev vocab: 1552\n",
            "Overlap len:  848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vc6JHTDErJv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}